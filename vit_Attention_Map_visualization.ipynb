{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import optim\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from torchvision import utils\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d34a89e49942e3a59000fc6d742094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import timm\n",
    "model = timm.create_model('vit_base_patch16_224', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset not found or corrupted. You can use download=True to download it",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose(\n\u001b[1;32m      2\u001b[0m     [transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m      3\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m,\u001b[38;5;241m224\u001b[39m)),\n\u001b[1;32m      4\u001b[0m      transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m), (\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m))])\n\u001b[1;32m      6\u001b[0m path2data \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 7\u001b[0m trainset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTL10\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath2data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m testset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mSTL10(path2data, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[1;32m     10\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/workspace/envs/LoRA/lib/python3.9/site-packages/torchvision/datasets/stl10.py:62\u001b[0m, in \u001b[0;36mSTL10.__init__\u001b[0;34m(self, root, split, folds, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload()\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# now load the picked numpy arrays\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels: Optional[np\u001b[38;5;241m.\u001b[39mndarray]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dataset not found or corrupted. You can use download=True to download it"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Resize((224,224)),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "path2data = './'\n",
    "trainset = datasets.STL10(path2data, split='train', download=False, transform=transform)\n",
    "testset = datasets.STL10(path2data, split='test', download=False, transform=transform)\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "# trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "# testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchPositionalEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels = 3, patch_resolution = 16, flatten_dimensions = 16*16*3, img_size=224, batch_size = 128):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.flatten = nn.Sequential(\n",
    "        #     # (h p1) = 224, p1 = 16 then h = 14\n",
    "        #     Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=self.patch_resolution, p2=self.patch_resolution),\n",
    "        #     nn.Linear(patch_resolution * patch_resolution * in_channels, flatten_dimensions)\n",
    "        # )\n",
    "\n",
    "        self.flatten = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, flatten_dimensions, patch_resolution, stride=patch_resolution),\n",
    "            Rearrange('b e (h) (w) -> b (h w) e')\n",
    "        )\n",
    "\n",
    "        self.class_token = nn.Parameter(torch.randn(1, 1, flatten_dimensions))# len == N+1, Positional Embedding\n",
    "        self.positional_Embedding = nn.Parameter(torch.randn(1, (img_size//patch_resolution)**2+1, flatten_dimensions)) \n",
    "\n",
    "    def forward(self, x):\n",
    "        b = x.shape[0]\n",
    "        x = self.flatten(x) # batch size * 3 * H * W => batchsize * N(HW//PP) * PPC\n",
    "        cls_tokens = repeat(self.class_token, '() n e -> b n e', b=b)\n",
    "        x = torch.concat([cls_tokens, x], dim = 1)\n",
    "        x += self.positional_Embedding\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(16, 3, 224, 224).to(device)\n",
    "patch_embedding = PatchPositionalEmbedding(in_channels = 3, patch_resolution = 16, flatten_dimensions = 16*16*3, img_size=224, batch_size = 16).to(device)\n",
    "patch_output = patch_embedding(x)\n",
    "print('[batch, 1+num of patches, emb_size] = ', patch_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiHeadAttention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedded_patches = 16*16*3, num_heads=8, dropout=0):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.embedded_patches = embedded_patches\n",
    "        self.LN = nn.LayerNorm(embedded_patches)\n",
    "        self.qkv = nn.Linear(embedded_patches, embedded_patches*3)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(embedded_patches, embedded_patches)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x_norm = self.LN(x)\n",
    "\n",
    "        # split keys, queries and values in num_heads\n",
    "        qkv = rearrange(self.qkv(x), 'b n (h d qkv) -> qkv b h n d', h= self.num_heads, qkv = 3) \n",
    "        queries, keys, values = qkv[0], qkv[1], qkv[1]\n",
    "        # sum up over the last axis, b,h,197,197\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_head, query_len, key_len\n",
    "        \n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "        \n",
    "        scaling = self.embedded_patches ** (1/2)\n",
    "        att = F.softmax(energy, dim=-1) / scaling\n",
    "        att = self.att_drop(att)\n",
    "        # sum up over the third axis\n",
    "        out = torch.einsum('bhal, bhlv -> bhav', att, values) # 197x91\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.projection(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_output = torch.randn(16, 197, 768).to(device)\n",
    "MHA = MultiHeadAttention(embedded_patches=16*16*3, num_heads=8, dropout=0).to(device)\n",
    "MHA_output = MHA(patch_output)\n",
    "print(MHA_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the residual addition.\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclassing nn.Sequential to avoid writing the forward method.\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size, expansion=4, drop_p=0.):\n",
    "        super().__init__()\n",
    "        self.LN = nn.LayerNorm(emb_size)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x_norm = self.LN(x)\n",
    "        x = self.ffn(x_norm)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create the Transformer Encoder Block\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size=48, drop_p=0., forward_expansion=4, forward_drop_p=0., **kwargs):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                MultiHeadAttention(emb_size, **kwargs),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                FeedForwardBlock(emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            ))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TransformerEncoder consists of L blocks of TransformerBlockb\n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth=12, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ClassificationHead which gives the class probability\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size=12, n_classes = 10):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.n_classes = n_classes\n",
    "        self.norm = nn.LayerNorm(self.emb_size, self.emb_size)\n",
    "        self.fc = nn.Linear(self.emb_size, self.n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = reduce(x, 'b n e -> b e', reduction='mean')\n",
    "        # x = torch.mean(x.view(x.size(0), x.size(1), -1), dim=1)\n",
    "        x = self.norm(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ViT architecture\n",
    "class ViT(nn.Sequential):\n",
    "    def __init__(self, in_channels=3, patch_resolution=16, flatten_dimensions=16*16*3, img_size=224, depth=12, n_classes=10, **kwargs):\n",
    "        super().__init__(\n",
    "            PatchPositionalEmbedding(in_channels, patch_resolution, flatten_dimensions, img_size, img_size),\n",
    "            TransformerEncoder(depth, emb_size=flatten_dimensions, **kwargs),\n",
    "            ClassificationHead(flatten_dimensions, n_classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT().to(device)\n",
    "summary(model, (3,224,224), device=device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 150 # 반복 수는 150\n",
    "cnt = 0      # early stopping을 적용하기 위해 만들어놓은 cnt\n",
    "\n",
    "model = ViT().to(device)\n",
    "criterion = nn.CrossEntropyLoss() # Cost Function으로 CrossEntropy 사용 -> 실습 때 배운 Cost Function이 CrossEntropy를 주로 사용했기 때문\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001) # 기본적인 Adam 사용 후 lr 및 Optimizier 변경 예정\n",
    "model.to(device) # model을 gpu에 올리기\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model).to(device)\n",
    "\n",
    "# train 및 val Loss 저장 d\n",
    "train_loss = torch.zeros(epochs)\n",
    "val_loss = torch.zeros(epochs)\n",
    "\n",
    "# train 및 val Accuracy 저장 \n",
    "train_acc = torch.zeros(epochs)\n",
    "val_acc = torch.zeros(epochs)\n",
    "\n",
    "# 초기 Loss값은 무한대\n",
    "valid_loss_min = np.Inf\n",
    "valid_acc_max = 0\n",
    "\n",
    "# epochs 수만큼 학습 진행\n",
    "for epoch in range(epochs):\n",
    "    # train mode 지정\n",
    "    model.train()\n",
    "    \n",
    "    # trainloader에서 값을 불러옵니다. \n",
    "    # trainloader의 구성 : inputs, labels \n",
    "    # inputs : 3x224x224 이미지\n",
    "    # labels : 해당 이미지의 대한 정답값\n",
    "    for inputs, labels in tqdm(trainloader):\n",
    "        \n",
    "        # input 값과 labels값을 GPU에 올려서 연산\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad() # optimizer 초기화 -> 모든 gradient를 초기화 시켜줌으로써 이전에 사용했던 기울기에 더해지지않고 새로 구하게 됨\n",
    "        \n",
    "        logits = model(inputs) #logits은 모델이 예측한 값으로 \n",
    "        loss = criterion(logits, labels) # criterion은 이전에 정의한 CrossEntropy를 통해 Loss 계산\n",
    "        loss.backward() # backward를 통해 역전파 실행 계산된 loss를 가지고 모델의 파라미터 개선\n",
    "        optimizer.step() # optimizer.step()을 통해 개선된 파라미터 적용\n",
    "\n",
    "        train_loss[epoch] += loss.item() # 에포크당 train_loss 누적\n",
    "        \n",
    "        ps = F.softmax(logits, dim=1) # softmax함수를 통한 정규화 (0 ~ 1) 사이의 확률로 만들어줌\n",
    "        top_p, top_class = ps.topk(1, dim=1) # topk를 통해 가장 높은 한개를 뽑음\n",
    "        equals = top_class == labels.reshape(top_class.shape)   # 일치하는지 확인.\n",
    "        train_acc[epoch] += torch.mean(equals.type(torch.FloatTensor)).item()  # 정확도 계산을 위해 float로 타입 변환 후 mean 계산.\n",
    "        \n",
    "    # Loss의 평균을 구하기\n",
    "    train_loss[epoch] /= len(trainloader)\n",
    "    train_acc[epoch] /= len(trainloader)\n",
    "\n",
    "    \n",
    "    # valid , 모델 검증 시간\n",
    "    model.eval()   #dropout Layer와 BatchNormLayer는 eval과정에서 필요하지 않기 때문\n",
    "    with torch.no_grad():  # validation 과정 no_grad()를 통해 Gradient 계산 안함.\n",
    "        # validloder에 넣어둔 값 들고오기\n",
    "        for inputs, labels in tqdm(testloader):\n",
    "            # 위에서 train 후 validation 실행. train한번 당 validation 1번 실행.\n",
    "            inputs, labels = inputs.to(device), labels.to(device) # 우선 GPU에 validation set 올리기\n",
    "            logits = model.forward(inputs) # validation set인 input값을 넣어서 실행 \n",
    "            batch_loss = criterion(logits, labels) # validation set의 Loss 계산\n",
    "\n",
    "            val_loss[epoch] += batch_loss.item() # Loss 값 누적\n",
    "\n",
    "            # Calculate accuracy\n",
    "            ps = F.softmax(logits, dim=1) # 확률값 구하기 (0~1) 사이로 정규화가 됨\n",
    "            top_p, top_class = ps.topk(1, dim=1) # 가장 높은값 하나를 고르는데 그 값과 idx가져옴\n",
    "            equals = top_class == labels.view(*top_class.shape) # 일치하는지 확인 \n",
    "            val_acc[epoch] += torch.mean(equals.type(torch.FloatTensor)).item() # # 정확도 계산을 위해 float로 타입 변환 후 mean 계산.\n",
    "            # valid_acc[epoch] += torch.mean(equals.type(torch.float)).detach().cpu()\n",
    "            \n",
    "    # validation Loss 및 accuracy 평균냄\n",
    "    val_loss[epoch] /= len(testloader)\n",
    "    val_acc[epoch] /= len(testloader)\n",
    "\n",
    "    ##################### PRINT LOSS & ACC #####################\n",
    "    print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "          f\"Train loss: {train_loss[epoch]:.3f}.. \"\n",
    "          f\"Train acc: {train_acc[epoch]:.3f}.. \"\n",
    "          f\"val loss: {val_loss[epoch]:.3f}.. \"\n",
    "          f\"val accuracy: {val_acc[epoch]:.3f}\")\n",
    "\n",
    "    ##################### 최적의 모델 저장 #####################\n",
    "    if val_acc[epoch] >= valid_acc_max:\n",
    "        print('Validation acc increased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_acc_max,\n",
    "        val_acc[epoch]))\n",
    "        torch.save(model.module.state_dict(), 'model_best.pt')\n",
    "        valid_acc_max = val_acc[epoch]\n",
    "\n",
    "        # 가장 낮은 Loss값을 가지게 된다면 Early Stopping count 초기화\n",
    "        cnt = 0\n",
    "\n",
    "    # 20번 이상 Loss 개선이 안된다면 종료\n",
    "    ############# Early Stopping #############\n",
    "    if cnt >= 10:\n",
    "        print(\"Early Stopping\")\n",
    "        break\n",
    "            \n",
    "    cnt+=1 #Loss 개선 실패\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "model = timm.create_model('vit_base_patch16_224', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = datasets.STL10(\"./\", split='test', download=True, transform=transforms.Compose([\n",
    "                                                                                transforms.Resize((224,224)),\n",
    "                                                                                 transforms.ToTensor()\n",
    "                                                                            ]))\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT2(nn.Sequential):\n",
    "    def __init__(self, in_channels=3, patch_resolution=16, flatten_dimensions=768, img_size=224, depth=12, n_classes=10, **kwargs):\n",
    "        super().__init__(\n",
    "            # PatchPositionalEmbedding(in_channels, patch_resolution, flatten_dimensions, img_size, img_size),\n",
    "            MultiHeadAttention(emb_size=768, num_heads=8, dropout=0)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT Attention Map visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. transfer timm model weight for Custom ViT model\n",
    "2. Attention map viuslization using first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = ViT2().to(device)\n",
    "model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "with torch.no_grad():\n",
    "    for i, e2 in enumerate(model.parameters()):\n",
    "        if i == 6: w = e2\n",
    "        elif i == 7:  w2 = e2\n",
    "    start = 0\n",
    "    for j, e in enumerate(model2.parameters()):\n",
    "        if j%2 == 0: e = w[start*768:(start+1)*768, :]\n",
    "        else: \n",
    "            e = w2[start*768:(start+1)*768]\n",
    "            start +=1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## layer 1, head = 7\n",
    "## pic = [Original image, Query, Key, Value, Attention output, Visualize]\n",
    "- visualized image : image Normalize and shift -0.9 (to make Correlated Attention output bright)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,10))\n",
    "import cv2\n",
    "\n",
    "with torch.no_grad():  # validation 과정 no_grad()를 통해 Gradient 계산 안함.\n",
    "    # validloder에 넣어둔 값 들고오기\n",
    "    for inputs, labels in testloader:\n",
    "        # 위에서 train 후 validation 실행. train한번 당 validation 1번 실행.\n",
    "        inputs, labels = inputs.to(device), labels.to(device) # 우선 GPU에 validation set 올리기        \n",
    "        for idxs in range(1, 11):\n",
    "            plt.subplot(6, 10, idxs)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(inputs[idxs].cpu().T)\n",
    "        inputs_r = rearrange(inputs, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=16, p2=16)\n",
    "        logits = model2.forward(inputs_r) # validation set인 input값을 넣어서 실행\n",
    "        print(logits.__len__())\n",
    "        heads = 7\n",
    "        # print(logits.__len__())\n",
    "        # print(logits[0].shape) \n",
    "        for idxs in range(1, 11):\n",
    "            # img = logits[1][idxs].reshape(197, 768)[1: ].reshape(3,224,224)\n",
    "            img = logits[0][idxs][heads][:, 0].reshape(14,14)\n",
    "            plt.subplot(6, 10,idxs+10)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(img.cpu().numpy().T, cmap = 'gray')\n",
    "        for idxs in range(1, 11):\n",
    "            # img = logits[1][idxs].reshape(197, 768)[1: ].reshape(3,224,224)\n",
    "            img = logits[1][idxs][heads][:, 0].reshape(14,14)\n",
    "            plt.subplot(6, 10,idxs+20)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(img.cpu().numpy().T, cmap = 'gray')\n",
    "        for idxs in range(1, 11):\n",
    "            # img = logits[1][idxs].reshape(197, 768)[1: ].reshape(3,224,224)\n",
    "            img = logits[2][idxs][heads][:, 0].reshape(14,14)\n",
    "            # img_color = cv2.resize(img.cpu().numpy(), (224,224))\n",
    "            plt.subplot(6, 10,idxs+30)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(img.cpu().numpy().T, cmap = 'gray')\n",
    "\n",
    "        for idxs in range(1, 11):\n",
    "            # img = logits[1][idxs].reshape(197, 768)[1: ].reshape(3,224,224)\n",
    "            img = logits[3][idxs][heads][:, 0].reshape(14,14)\n",
    "            # img_color = cv2.resize(img.cpu().numpy(), (224,224))\n",
    "            # img_color = cv2.cvtColor(img_color, cv2.COLOR_GRAY2BGR)\n",
    "            plt.subplot(6, 10,idxs+40)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(img.cpu().numpy().T, cmap = 'gray')\n",
    "            # plt.imshow(np.max(img_color.mean(), img_color) + inputs[idxs].cpu().numpy().T, cmap = 'gray')\n",
    "\n",
    "        for idxs in range(1, 11):\n",
    "            # img = logits[1][idxs].reshape(197, 768)[1: ].reshape(3,224,224)\n",
    "            img = logits[3][idxs][heads][:, 0].reshape(14,14)\n",
    "            img_color = cv2.resize(img.cpu().numpy(), (224,224))\n",
    "            img_color = cv2.rotate(img_color, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "            img_color = cv2.cvtColor(img_color, cv2.COLOR_GRAY2BGR)\n",
    "            img_color = cv2.flip(img_color, 0)\n",
    "\n",
    "            plt.subplot(6, 10,idxs+50)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(inputs[idxs].cpu().numpy().T - ((img_color-img_color.min())/(img_color.min() - img_color.max())) - 0.9, cmap = 'gray')\n",
    "            # plt.imshow(img_color[:, :, 0], cmap = 'gray')\n",
    "        plt.show()\n",
    "\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_medical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fabe9fbab7fcb9af61e2f91f1628924abd8d84c01e6e010f03f0a45230933de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
